{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the CSV file (data.csv) as a DataFrame\n",
    "ufc_df = pd.read_csv(\"Resources/data.csv\")\n",
    "ufc_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non-beneficial columns\n",
    "ufc_df = ufc_df.drop(\n",
    "    columns=[\n",
    "        \"BPrev\",\n",
    "        \"RPrev\",\n",
    "        \"BStreak\",\n",
    "        \"B_Location\",\n",
    "        \"R_Location\",\n",
    "        \"Event_ID\",\n",
    "        \"Fight_ID\",\n",
    "        \"B_ID\",\n",
    "        \"R_ID\",\n",
    "        \"B_HomeTown\",\n",
    "        \"R_HomeTown\",\n",
    "        \"Date\",\n",
    "    ]\n",
    ")\n",
    "ufc_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only wins and losses (i.e., Red & Blue)\n",
    "ufc_df = ufc_df.loc[(ufc_df.winner == \"blue\") | (ufc_df.winner == \"red\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Imputation transformer for completing missing values.\n",
    "# Standardize features by removing the mean and scaling to unit variance with `StandardScalar()`.\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, selector(dtype_exclude=\"object\")),\n",
    "        (\"cat\", categorical_transformer, selector(dtype_include=\"object\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", LogisticRegression(max_iter=500, random_state=1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X = ufc_df.drop(\"winner\", axis=1)\n",
    "y = ufc_df[\"winner\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Diagram of Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Grid Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search can also be performed on the different preprocessing steps defined in the `ColumnTransformer` object, together with the classifierâ€™s hyperparameters as part of the `Pipeline`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for both the imputer strategy of the numeric preprocessing and the regularization parameter of the logistic regression using GridSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\n",
    "        \"mean\",\n",
    "        \"median\",\n",
    "        \"most_frequent\",\n",
    "        \"constant\",\n",
    "    ],\n",
    "    \"classifier__C\": [0.1, 1.0, 10, 100],\n",
    "    \"classifier__solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)  # `n_jobs=-1`\n",
    "grid_search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `grid_search.fit` triggers the cross-validated search for the best hyper-parameters combination:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\")\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.get_params().keys()\n",
    "\n",
    "# for parameter in clf.get_params():\n",
    "#     print(parameter)\n",
    "\n",
    "# clf.get_params()\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# Solver: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internal cross-validation scores obtained by those parameters is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Internal CV score: {grid_search.best_score_:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also introspect the top grid search results as a pandas dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results = cv_results.sort_values(\"mean_test_score\", ascending=False)\n",
    "cv_results[\n",
    "    [\n",
    "        \"mean_test_score\",\n",
    "        \"std_test_score\",\n",
    "        \"param_preprocessor__num__imputer__strategy\",\n",
    "        \"param_classifier__C\",\n",
    "        \"param_classifier__solver\",\n",
    "    ]\n",
    "].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyper-parameters have be used to re-fit a final model on the full training set. Evaluate that final model on held out test data that was not used for hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    (\n",
    "        \"best logistic regression from grid search: %.3f\"\n",
    "        % grid_search.score(X_test, y_test)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression without Column Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split features and target arrays\n",
    "\n",
    "# Binary encoding\n",
    "dummies_df = pd.get_dummies(ufc_df)\n",
    "X = dummies_df.drop(columns=[\"winner_blue\", \"winner_red\"])\n",
    "X = X.fillna(0)\n",
    "\n",
    "y = ufc_df[\"winner\"]\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=1\n",
    ")\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Create and train the Logistic Regression Model\n",
    "clf = LogisticRegression(solver=\"lbfgs\", max_iter=200)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict Outcomes\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# Print accuracy score\n",
    "print(f\" Logistic regression model accuracy: {accuracy_score(y_test,y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Logistic Regression with Subset of Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"Max_round\",\n",
    "    \"B_Age\",\n",
    "    \"B_Height\",\n",
    "    \"B_Weight\",\n",
    "    \"R_Age\",\n",
    "    \"R_Height\",\n",
    "    \"R_Weight\",\n",
    "]\n",
    "categorical_features = [\"winby\"]\n",
    "target_value = [\"winner\"]\n",
    "\n",
    "subset_features = numeric_features + categorical_features + target_value\n",
    "\n",
    "# Select subset of features\n",
    "ufc_subset_df = ufc_df[subset_features]\n",
    "\n",
    "# Drop null (?)\n",
    "# ufc_subset_df = ufc_subset_df.dropna(how=\"any\", axis=\"rows\")\n",
    "\n",
    "# Reset index\n",
    "ufc_subset_df = ufc_subset_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encoding\n",
    "dummies_df = pd.get_dummies(ufc_subset_df)\n",
    "X = dummies_df.drop(columns=[\"winner_blue\", \"winner_red\"])\n",
    "X = X.fillna(0)\n",
    "\n",
    "y = ufc_subset_df[\"winner\"]\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=1\n",
    ")\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Create and train the Logistic Regression Model\n",
    "clf = LogisticRegression(solver=\"lbfgs\", max_iter=200)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict Outcomes\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# Print accuracy score\n",
    "print(f\" Logistic regression model accuracy: {accuracy_score(y_test,y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: StandardScaler on get_dummies()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Should StandScalar() be applied to non-numerical categories that got encoded? (Ex. OneHotEncoder() or get_dummies())\n",
    "\n",
    "# We know that there are 4 columns that are going to be encoded with get_dummies() since they are `object` dtype\n",
    "# `columns=` -> Column names in the DataFrame to be encoded. If columns is None then all the columns with object or category dtype will be converted.\n",
    "display(ufc_df.dtypes.value_counts())\n",
    "\n",
    "# Should `dummy_na=True`? -> Add a column to indicate NaNs, if False NaNs are ignored.\n",
    "display(ufc_df.select_dtypes(include=\"object\").isnull().sum())\n",
    "\n",
    "\n",
    "# We are encoding the object variables into uint8\n",
    "dummies_df = pd.get_dummies(ufc_df)\n",
    "display(dummies_df.dtypes.value_counts())\n",
    "\n",
    "# Should StandScalar() be applied to non-numerical categories that got encoded? (Ex. OneHotEncoder() or get_dummies())\n",
    "# dummies_df.select_dtypes(include=\"uint8\").columns.tolist()\n",
    "# dummies_df.select_dtypes(exclude=\"uint8\").columns.tolist()\n",
    "\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: tpot AutoML\n",
    "\n",
    "Dirty run of getting `tpot` to run with subset of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try the following:\n",
    "# TODO: Built-in TPOT configurations -> https://epistasislab.github.io/tpot/using/#built-in-tpot-configurations\n",
    "# TODO: Customizing TPOT's operators and parameters -> https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters\n",
    "# TODO: Neural Networks in TPOT -> https://epistasislab.github.io/tpot/using/#neural-networks-in-tpot-tpotnn\n",
    "# TODO: Load without pre-selected subset of features\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ufc_subset_df = ufc_df.convert_dtypes()\n",
    "\n",
    "numeric_features = [\n",
    "    \"Max_round\",\n",
    "    \"B_Age\",\n",
    "    \"B_Height\",\n",
    "    \"B_Weight\",\n",
    "    \"R_Age\",\n",
    "    \"R_Height\",\n",
    "    \"R_Weight\",\n",
    "]\n",
    "categorical_features = [\"winby\"]\n",
    "target_value = [\"winner\"]\n",
    "subset_features = numeric_features + categorical_features + target_value\n",
    "\n",
    "# Conduct analysis only on subset of features\n",
    "ufc_subset_df = ufc_df[subset_features]\n",
    "\n",
    "# Drop rows that contain null values\n",
    "# ufc_subset_df.isnull().sum()\n",
    "ufc_subset_df = ufc_subset_df.dropna(axis=0, how=\"any\")\n",
    "\n",
    "# Encode categorical features with `get_dummies`\n",
    "ufc_subset_df = pd.get_dummies(ufc_subset_df, columns=categorical_features)\n",
    "\n",
    "# \"Encode\" text as numbers\n",
    "ufc_subset_df[\"winner\"] = np.where(\n",
    "    ufc_subset_df[\"winner\"] == \"red\", 0, 1\n",
    ")  # 0 = value if true (red), 1 = value if not true (blue)\n",
    "\n",
    "# Reset index\n",
    "ufc_subset_df = ufc_subset_df.reset_index(drop=True)\n",
    "\n",
    "# Set features (X) and target (y)\n",
    "X = ufc_subset_df.drop(\"winner\", axis=1)\n",
    "y = ufc_subset_df[\"winner\"]\n",
    "\n",
    "# Split data into test/train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Run tpot classifier on light mode\n",
    "tpot = TPOTClassifier(\n",
    "    generations=5, population_size=20, verbosity=2, config_dict=\"TPOT light\"\n",
    ")\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export(\"tpot_TPOTClassifier_light_pipeline.py\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d13ba43a92555a83a91537475403bd0fdb075d2524ba3a00b200fd1cb72e7c9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
